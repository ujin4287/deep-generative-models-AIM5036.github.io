---
layout: post
title: "WGAN"
date: 2022-11-12
author: Khan Osama, Jeonghyeon Kim
categories: GAN
tags: WGAN, GAN
use_math: true
---

# WGAN

# Introduction

WGAN 이란?
적대적 신경 생성망 GAN의 비용함수를 Wasserstein distance로 설정하여 최적화를 진행하는 신경망이다.

기존의 GAN은 이미지가 복잡해 짐(고차원화) 에 따라 학습의 난이도가 아주 올라가게 되고 이로 인해
학습이 매우 불안정 해지는 문제가 발생하게 되었다.  
WGAN에서는 학습의 불안정성의 원인을 metric(거리)이라 보고 이것을 수학적으로 해결하고자 하였다.

![Untitled](/assets/WGAN/KL.png)  
기존의 두 분포 사이의 거리를 측정하기 위해 주로 쓰이는 KLD는 위수식과 같다.  
하지만 KLD와 같은 metric은 분모에 위치하는 $P_g(x) = 0$ 이고,
$P_r(x) \ne 0$ 인 곳이 발생하게 된다면 발산하게 되는 문제점이 생긴다.  
본 논문에서는 저차원에서 주로 이러한 문제가 발생한다고 서술하고 있다.

![Untitled](/assets/WGAN/long_distribution.png)  
또한 위 그림과 두 분포가 겹치지 않는 다면, 즉 support가 겹치지 않은 상황에서 역시
두 분포 사이의 거리가 상당히 멀어 좋지 않은 gradient feedback을 주게 되고
유의미한 학습이 일어나기가 힘들다 는 문제가 있다.

따라서 본 논문에서는 모델의 학습이 얌전히 수렴하고, 매끄러운 metric을 찾는 것을 목표로 한다.

# Different Distances

어떤 metric이 더 GAN의 학습에 적절한 한지 알아보기 위해 다양한
Distance 방법들을 살펴 본다.

## The Total Variation(TV) Distance

![Untitled](/assets/WGAN/TVD.png)  
위의 식은 Total Variation (TV) distance를 나타낸다. 여기서 $sup$ 은 supremum 최소상한라는 의미이며
어떤 집합의 상한 중 가장 작은 값을 의미한다.

TVD는 두확률 분포의 가능한 측정 값들 중 차이가 가장 큰 값 $sup$ 으로 정의된다.

![Untitled](/assets/WGAN/TVD_example.png)  
위 그림에서 처럼, 가능한 사건들을 A로 잡았을 때, 두 분포를 비교 했을떄 측정 값은
서로 다른값을 가지게 된다. 따라서 두 분포중 큰 값을 TVD로 한다.

![Untitled](/assets/WGAN/TVD_zero.png)
TVD 에서 만약 두 분포가 겹치지 않는다면 즉 support가 공집합이라면,
TVD는 1을 가지게 된다.

## The Kullback-Leibler Divergence

![Untitled](/assets/WGAN/KL.png)  
위에서 언급 했던 대로 KLD은 분모에 위치하는 $P_g(x) = 0$ 이고,
$P_r(x) \ne 0$ 인 곳이 발생하게 된다면 발산하게 되는 문제점이 생긴다.

## The Jensen-Shannon Divergence

![Untitled](/assets/WGAN/JSD.png)  
JSD는 KLD를 이용하여 간략하게 표현 될 수 있다.  
만약 두 분포의 support 가 겹치지 않는다면
![Untitled](/assets/WGAN/support.png)  
으로 나타나게 되고  
![Untitled](/assets/WGAN/JS_log2.png)  
위 수식처럼 $log2$ 로 두 분포사이의 거리가 고정된다.
따라서 JSD는 발산하지는 않지만 $log2$ 처럼 상수 값만 가지게 되므로
좋은 피드백을 주는 것이 어렵다.

## The Earth Mover (EM) Distance (Wasserstein-1)

![Untitled](/assets/WGAN/EM.png)  
위 식은 EMD 즉 Wasserstein 을 보여준다.  
EM은 $P_r$ 을 $P_g$ 로 변경하기 위한 여러 경우 중
가장 최적의 transform plan에 대한 cost를 구할 수 있도록 해준다.

# EM의 타당성 설명

![Untitled](/assets/WGAN/EM1.png)  
본 논문에서는 위에서 논의한 다양한 distance 들과 EM을 비교하여 EM의 타당성을 이야기한다.
기존의 GAN 논문에서 Disciminator 와 Gnerator가 학습할 떄 EM을 제외한 다른 metric을 사용한다면,  
$\theta \ne 0$ 일 경우 두 분포가 겹치지 않게 되고 무의미한 값들을 구하게 된다.  
하지만 EM의 경우 공식에 따라 두 점사이의 거리는 항상 $\left\vert \theta \right\vert$ 로 나타나게된다.

![Untitled](/assets/WGAN/EM_graph.png)  
위 그림은 논문에서 JSD와 EM의 gradient를 비교하여 보여주는 그림이다.  
오른쪽의 그림은 JSD를 보여주는데 JSD의 경우 $\theta = 0$ 이외에선 상수로 일정하게 나타나고,  
$0$ 일 떄는 $0$ 으로 나타나게 된다. 따라서 유의미한 gradient를 얻기가 힘든 것을 알 수 있다.  
왼쪽 첫번 째 그림의 EM의 경우 좀 더 유의미한 gradient를 내보내는 것을 볼 수 있다.
따라서 EM 즉 W distance가 GAN의 학습에 좀더 유용하다는 것을 알 수 있다.

# Wasserstein GAN

본 논문에서는 Wasserstein 1 Distance를 GAN의 Loss function으로 정의 했다.  
![Untitled](/assets/WGAN/ori_DM.png)  
의 EM 공식을 살펴보면 inf가 나오는 해당 부분의 계산이 intractable 하다.  
따라서 논문에서는 Kantorovich-Rubinstein duality를 사용하여 식을 변경한다.  
![Untitled](/assets/WGAN/W_ch.png)  
이렇게 식을 정리하게 되면 위 수식과 같이된다.  
위 수식에서 $\left\vert\left\vert f \right\vert\right\vert_L \leq 1$은 $f$ 가 1-립쉬츠 함수,  
즉 임의의 두점 사이의 평균변화율이 1이 넘지 않는 함수 라는 뜻을 의미한다.  
위의 정리된 수식을 사용하면 $f$ 만 알아 낼 수 있다면 $W$ 값을 얻을 수 있다.

본 논문에서는 $w$를 추가로 변수로 사용하여 $f_w$ 를 업데이트 하고 있다.  
기존 위의 수식에서 $P_\theta$ 를 $g\theta(z)$ 로 바꾸어 주면 아래 수식과 같은 식이 나온다.

![Untitled](/assets/WGAN/DM_ch2.png)  
위의 수식의 모양이 GAN의 Loss 함수와 비슷한 모습을 보이는 것을 알 수 있다.

본 논문에선 립시츠 조건을 만족하도록 discriminator (Critic)의 파라미터 W가 일정범위 [-0.01, 0.01] 내에
있도록 강제로 제한한다.  
이과정을 clipping이라고 부른다. 이 방법에 대해서 논문에선 cleary terrible way 라고할 정도로
좋지 않게 표현하고 있다. 하지만 실험 결과 해당 방법이 너무 간결하고 좋은 결과를 보여 사용했다고
언급하고 있다.

![Untitled](/assets/WGAN/%EC%B0%A8%EC%9D%B4%EC%A0%90GAN.png)  
위 그림에서는 기존 GAN과 WGAN의 discriminator (Critic)이 어떻게 다르게 동작하는지 보여준다.  
그림에서 빨간색은 GAN의 discriminator를 나타내는데, 빠르게 fake와 real을 잘 구분할 수 있게 되지만
discriminator가 결국 gradient vanishing이 발생하게 된다.  
하지만 하늘색 선의 WGAN의 경우 critic이 gradient vanishing에 빠지지 않고 linear 하게 gradient가 잘 발생된다.  
따라서 model collapse가 발생할 수 없는 상황이 된다.

# Empirical Results

![Untitled](/assets/WGAN/result_DM.png)
위 그림은 EMD를 사용하여 학습을 진행할 때 나오는 이미지와 estimation 간의 상관관계를 보여준다.  
그림을 살펴보면 W 값은 이미지의 퀄리티가 좋아 질 수록 W가 점점 감소하는 것을 볼수 있고, 마지막
사진에선 망가진 이미지가 나올 때는 W가 줄어들지 않는 모습을 보인다.

![Untitled](/assets/WGAN/result_JS.png)
위 그림은 JSD를 사용하여 학습을 진행항 때 나오는 이미지와 estimation 간의 상관관계를 보여준다.  
JSD의 같은 경우 이미지의 품질에 상관없이 estimation이 $log2$ 즉 약 0.69로 고정되는 모습을 보인다.  
또한 마지막 망가진 이미지의 사진에선 오히려 내려갔다 올라가는 모습을 보이기도한다.

# Conclusion

해당 논문에서는 dicriminator와 Generator 간의 균형을 유지하며 학습하기 어려운  
GAN이 가진 문제점을 해결하기 위해 Discriminator 대신 새로운 Critic을 정의하여 사용하며,
Critc을 구하기 위해 Wasserstein distance (EMD)를 사용한다.  
EMD를 통해 GAN에서 주로 발생하는 문제인 mode collapse를 해결 하고 있다는 점에서 인상적이다.
하지만 본 논문에선 립시츠 조건을 만족하기 위해 weight clipping 방법을 사용하고 있는데,
해당 법이 좋지 않음에도 사용하고, 추후에 어떻게 개선시켜야 할지에 대해 논의하고 있지 않는 점이
아쉽다고 생각된다.
